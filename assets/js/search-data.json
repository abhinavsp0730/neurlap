{
  
    
        "post0": {
            "title": "TensorFlow callbacks in action",
            "content": "TensorFlow Callbacks in Action . . In layman terms, if I want to introduce callbacks, then it‚Äôs the controller by which you can control your plane. Without these controllers, you‚Äôre not having any control over the plane, and you‚Äôll crash. . Callbacks: from keras.io, a callback is an object that can perform actions at various stages of training (e.g., at the start or end of an epoch, before or after a single batch, etc.). . It means that callbacks are the functions by which you can perform a particular task during the training process of your model. So, what can you do with these callbacks? . You can perform a particular task after the starting and ending of the training/batch/ epochs. | You can periodically save the model states in the disk. | You can schedule the learning rate as per your task. | You can automatically stop the training when a particular condition becomes True. | And you can do anything during the training process by subclassing these callbacks. | For example, you can make your training output clean and colorful like this, pretty awesome, right? . . Tensorflow provides a wide range of callbacks under the base class ‚Äútf.keras.callbacks. ‚ÄúFor the full list of callbacks please visit TensorFlow‚Äôs website. . In this article, we‚Äôre going to cover some of the essential TensorFlow callbacks and how to use them to have full control over the training. . The context of this article are:- . custom callbacks by subclassing callback class. | Early stopping callback. | Model checkpoint callback. | ReduceOnPlateu callback. | Learning rate Scheduler. | Bonus package for making the output clean and colorful, as shown above. | But let‚Äôs first load the cats_vs_dogs dataset, I‚Äôve been using the very small subclass of the original dataset. And then, let‚Äôs define our model architecture using sequential API. Throughout this article, I‚Äôm using this dataset and this model architecture. . . . . Note:- This article is all about the TensorFlow callbacks and not for making a world-class ML model and for achieving the state-of-art result. So, throughout this article, ignore the loss and the metrics and try to focus on how to use these callbacks. The dataset is minimal, and it may overfit, but you can ignore all these things. . So, without further delay, let‚Äôs start learning about the callbacks mentioned above. . 1. Custom callbacks by subclassing callback class. . These callbacks come under the base class ‚Äútf.keras.callbacks.‚Äù By subclassing these callbacks, we can perform certain functions when the training/batch/epochs have started or ended. For this, we can override the function of callback classes. The name of these functions is self explain their behavior. For example def on_train_begin(), this means what to do when training will begin. Let‚Äôs see below how to override these functions. We can also, monitor logs and perform certain actions, generally at the starting or the ending of the training/batch/epochs. . . Output: . . 2. EarlyStopping Callback. . Suppose we don‚Äôt know about the callbacks, and you want to prevent the overfitting of the model caused by training our model into extra epochs(we‚Äôre not god so that we know at how many epochs our model is going to converge). So, we plot the val_loss vs. epochs graph and examine how many epochs it‚Äôs started overfitting the data. Then we‚Äôll re-train our model in less than that epoch number. What if I‚Äôll tell you don‚Äôt have to do this thing manually. Yes, you can do this by using EarlyStopping Callback. So, let‚Äôs see how one can use this callback. . First, import the callback, and then create the instance of the EarlyStopping callback and pass the arguments as per our needs. Lemme explain these arguements . . ‚Äúmonitor‚Äù you can pass the loss or the metric. Generally, we pass val_loss and monitor it. . | ‚Äúmin_delta‚Äù you can pass an integer in this argument. In simple words, you‚Äôre telling the callback that the model is not improving if it‚Äôs not decreasing more/less than the loss/metrics. . | ‚Äúpatience,‚Äù it means about how many epochs to wait. And after that, if there is no improvement seen in the model performance according to the value of ‚Äúmin delta,‚Äù then stop the training. . | ‚Äúmode‚Äù By default it‚Äôs set to ‚Äòauto‚Äô this comes handy when you‚Äôre dealing with the custom loss/metric. So, you can tell the callback whether the model is improving when its custom loss/metric is decreasing then set it to ‚Äúmin‚Äù or increasing then set it to ‚Äúmax.‚Äù . | . . 3. ReduceLROnPlateau. . This callback is used to reduce the learning rate if there is not any improvement in the loss/metric. . The arguments are: . ‚Äúmonitor‚Äù it‚Äôs set to that loss/metric as a string of which we are reducing the learning if it‚Äôll not improve. . | ‚Äúfactor‚Äù You can pass an integer in this argument, and say your current learning rate is LR, then if there is not any improvement seen in the monitored loss/metric, then the learning is going to decrease by that ‚Äúfactor.‚Äù i.e new learning rate = lr * factor . | ‚ÄúVerbose‚Äù You can set verbose =1 to see the learning rate at every epoch. Or verbose = 0 to disable it. . | . The argument min_delta and mode are the same as explained in the arguments of EarlyStopping Callback. . . 4. ModelCheckpoint . Let‚Äôs imagine you‚Äôre training a heavy model like Bert in colab, and it requires a lot of time for training. So, you started the model training and went for sleep. And then the next morning you wake up, and you open your colab. But you‚Äôll see the ‚ÄúRuntime Disconnect‚Äù message on your screen. Sounds like a nightmare tough? For this problem, ModelCheckpoint comes as a savior in our life. We can save the checkpoints at the end of every epoch. So, that we can load the weights or resume the training if something terrible happens while training. . So, let‚Äôs see how we can use this callback. We can save the model checkpoint in Keras h5/hd5 format or TensorFlow pb format. If you pass the argument ‚Äúfilepath= model.h5‚Äù(.h5 extension) it‚Äôll be saved in the Keras format or ‚Äúfilepath= model.p‚Äù(.pb extension) for saving in the TensorFlow model format. . Also, there are two options to save the checkpoint either you can save the entire architecture+weights or just the weights. You can do this by setting ‚Äúsave_only_weights=True‚Äù or ‚Äúsave_only_weights=False‚Äù . . 5. LearningRateScheduler . The simplest way to schedule the learning is to decrease the learning rate linearly from a large initial value to a small value. This allows large weight changes at the beginning of the the learning process and small changes or fine-tuning towards the end of the learning process. . Let‚Äôs see how to schedule the learning rate. For this, we have to define an auxiliary function that contains the rules for alternating the learning rate. And then we can simply pass the name of this auxiliary function to the argument of the object of the LearningRateScheduler class. . . Output: . . lastly here is the utility file to make training output cleaner and colorful. . . Resources . Repository on Github: . You can run all the code above on Google‚Äôs colab. .",
            "url": "https://abhinavsp0730.github.io/neurlap/markdown/2020/06/10/TENSORFLOW-CALLBACKS-IN-ACTION.html",
            "relUrl": "/markdown/2020/06/10/TENSORFLOW-CALLBACKS-IN-ACTION.html",
            "date": " ‚Ä¢ Jun 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Tf.estimator, a Tensorflow High-level API",
            "content": "Tf.estimator, a Tensorflow High-level API . . Now Tensorflow 2.0 has been officially released and it‚Äôs having two high-level deep learning APIs. The first one is tf.keras and another one is tf.estimator. You can see the list of TensorFlow‚Äôs Python API in the picture above. Some of you are familiar with building an ML model using Keras. But we‚Äôre not so familiar with tf.estimator (Assuming we refer to a beginner in ML). So let us understand tf.estimator. . The context of this article is: . Giving you an idea about what tf.estimator is all about. 2.What tasks we‚Äôve to follow while writing the TensorFlow program based on Estimators(pre-made Estimators). | Advantages . 4.Estimators capabilities. | We‚Äôre going to build and test a model by using tf.estimator that classifies iris flowers into there species. | What is tf.estimator? . An Estimator is TensorFlow‚Äôs high-level representation of a complete model, and it has been designed for easy scaling and asynchronous training. It‚Äôs used to train the neural network model and use them to predict new data. It‚Äôs a high-level API that sits on top of the low-level core TensorFlow API. One can use a pre-made estimator or custom estimator. . 1. Pre-made Estimators . Pre-made Estimators enable you to work at a much higher conceptual level than the base TensorFlow APIs. You no longer have to worry about creating the computational graph or sessions since Estimators handle all the ‚Äúplumbing‚Äù for you. Furthermore, pre-made Estimators let you experiment with different model architectures by making only minimal code changes. tf.estimator. DNNClassifier, for example, is a pre-made Estimator class that trains classification models based on dense, feed-forward neural networks. . 2. Custom estimator . The heart of every Estimator ‚Äî whether pre-made or custom ‚Äî is its model function, which is a method that builds graphs for training, evaluation, and prediction. When you are using a pre-made Estimator, someone else has already implemented the model function. When relying on a custom Estimator, you must write the model function yourself. In this model, we‚Äôre mainly dealing with pre-made estimators . Tasks for writing TensorFlow pre-made estimators. . . 1.Create one or more input functions. . Define the model‚Äôs feature columns. | Instantiate an Estimator, specifying the feature columns and various hyperparameters. | Call one or more methods on the Estimator object, passing the appropriate input function as the source of the data. | Later in this article, we‚Äôre going to implement the above tasks for iris classification. . Advantages . The tf.estimator provides some capabilities currently still under development for tf.keras. . #These are:- #1.We can conduct distributed training across multiple servers with the Estimators API #2.Full TFX integration. #TensorFlow Extended (TFX) is an end-to-end platform for deploying production ML pipelines. The tf.estimator is supported for fully TFX integration. . Estimators capabilities . Estimators provide the following benefits: . You can run Estimator-based models on a localhost or a distributed multi-server environment without changing your model. Furthermore, you can run Estimator-based models on CPUs, GPUs, or TPUs without recoding your model. . | Estimators provide a safely distributed training loop that controls how and when to: (a)load data (b)handle exceptions ¬©create checkpoint files and recover from failures (d)save summaries for TensorBoard . | Iris classifier using tf.estimator . We‚Äôre going to build an iris classifier using tf.estimator. The dataset we‚Äôre using is iris data set which is having four features sepal length, sepal width, petal length &amp; petal width and three labels Setosa, Versicolor &amp; Virginica. But first, we import all the dependencies . from __future__ import absolute_import, division, print_function, unicode_literals import tensorflow as tf import pandas as pd . And then we preprocess the data to perform the following task:- . (a)Create one or more input functions. (b)Define the model‚Äôs feature columns. (c )Instantiate an Estimator, specifying the feature columns and various hyperparameters. (d)Call one or more methods on the Estimator object, passing the appropriate input function as the source of the data. . Preprocessing the data . CSV_COLUMN_NAMES = [&#39;SepalLength&#39;, &#39;SepalWidth&#39;, &#39;PetalLength&#39;, &#39;PetalWidth&#39;, &#39;Species&#39;] SPECIES = [&#39;Setosa&#39;, &#39;Versicolor&#39;, &#39;Virginica&#39;] . Downloading the data set. . train_path = tf.keras.utils.get_file( &quot;iris_training.csv&quot;, &quot;https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv&quot;) test_path = tf.keras.utils.get_file( &quot;iris_test.csv&quot;, &quot;https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv&quot;) train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0) test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0) . Creating an input function . You must create input functions to supply data for training, evaluating, and prediction. . An input function is a function that returns a tf.data.Dataset object which outputs the following two-element tuple: . features ‚Äî A Python dictionary in which: (a)Each key is the name of a feature. (b)Each value is an array containing all of that feature‚Äôs values. label ‚Äî An array containing the values of the label for every example. We‚Äôre using pandas for building input pipeline . def input_fn(features, labels, training=True, batch_size=256): &quot;&quot;&quot;An input function for training or evaluating&quot;&quot;&quot; # Convert the inputs to a Dataset. dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels)) # Shuffle and repeat if you are in training mode. if training: dataset = dataset.shuffle(1000).repeat() return dataset.batch(batch_size) . Define the feature columns . A feature column is an object describing how the model should use raw input data from the features dictionary. When you build an Estimator model, we pass it a list of feature columns that describe each of the features you want the model to use. The tf.feature_column module provides many options for representing data to the model. . For Iris, the 4 raw features are numeric values, so we‚Äôll build a list of feature columns to tell the Estimator model to represent each of the four features as 32-bit floating-point values. Therefore, the code to create the feature column is: . # Feature columns describe how to use the input. my_feature_columns = [] for key in train.keys(): my_feature_columns.append(tf.feature_column.numeric_column(key=key)) . Instantiate an estimator . The Iris problem is a classic classification problem. Fortunately, TensorFlow provides several pre-made classifier Estimators, including: . a. tf.estimator.DNNClassifier for deep models that perform multi-class classification. b. tf.estimator.DNNLinearCombinedClassifier for wide &amp; deep models. c. tf.estimator.LinearClassifier for classifiers based on linear models. . For the Iris problem, tf.estimator.DNNClassifier seems like the best choice. Here‚Äôs how we instantiated this Estimator: . # Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each. classifier = tf.estimator.DNNClassifier( feature_columns=my_feature_columns, # Two hidden layers of 10 nodes each. hidden_units=[30, 10], # The model must choose between 3 classes. n_classes=3) . Train, Evaluate, and Predict . Train the model Train the model by calling the Estimator‚Äôs train method as follows: . # Train the Model. classifier.train( input_fn=lambda: input_fn(train, train_y, training=True), steps=5000) . Evaluate . Now that the model has been trained, you can get some statistics on its performance. The following code block evaluates the accuracy of the trained model on the test data: . eval_result = classifier.evaluate( input_fn=lambda: input_fn(test, test_y, training=False)) print(&#39; nTest set accuracy: {accuracy:0.3f} n&#39;.format(**eval_result)) . After evaluating it we‚Äôll get an accuracy of about 56% . Making predictions (inferring) from the trained model . You now have a trained model that produces good evaluation results. You can now use the trained model to predict the species of an Iris flower based on some unlabeled measurements. As with training and evaluation, you make predictions using a single function call: . # Generate predictions from the model expected = [&#39;Setosa&#39;, &#39;Versicolor&#39;, &#39;Virginica&#39;] predict_x = { &#39;SepalLength&#39;: [5.1, 5.9, 6.9], &#39;SepalWidth&#39;: [3.3, 3.0, 3.1], &#39;PetalLength&#39;: [1.7, 4.2, 5.4], &#39;PetalWidth&#39;: [0.5, 1.5, 2.1], } def input_fn(features, batch_size=256): &quot;&quot;&quot;An input function for prediction.&quot;&quot;&quot; # Convert the inputs to a Dataset without labels. return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size) predictions = classifier.predict( input_fn=lambda: input_fn(predict_x)) . The predict method returns a Python iterable, yielding a dictionary of prediction results for each example. The following code prints a few predictions and their probabilities: . for pred_dict, expec in zip(predictions, expected): class_id = pred_dict[&#39;class_ids&#39;][0] probability = pred_dict[&#39;probabilities&#39;][class_id] print(&#39;Prediction is &quot;{}&quot; ({:.1f}%), expected &quot;{}&quot;&#39;.format( SPECIES[class_id], 100 * probability, expec)) . We‚Äôll get an output like this . INFO:tensorflow:Calling model_fn. INFO:tensorflow:Done calling model_fn. INFO:tensorflow:Graph was finalized. INFO:tensorflow:Restoring parameters from /tmp/tmpy5w5zoj8/model.ckpt-5000 INFO:tensorflow:Running local_init_op. INFO:tensorflow:Done running local_init_op. Prediction is &quot;Setosa&quot; (73.0%), expected &quot;Setosa&quot; Prediction is &quot;Virginica&quot; (42.6%), expected &quot;Versicolor&quot; Prediction is &quot;Virginica&quot; (49.0%), expected &quot;Virginica&quot; . Refrences:- Tensorflow‚Äôs official Documentation . Hope you like this article . Do you know what, you can hit the clap button 50 times in medium? If you like this blog, show some love by doing claps. . .",
            "url": "https://abhinavsp0730.github.io/neurlap/markdown/2019/10/07/Tf.estimator-a-Tensorflow-High-level-API.html",
            "relUrl": "/markdown/2019/10/07/Tf.estimator-a-Tensorflow-High-level-API.html",
            "date": " ‚Ä¢ Oct 7, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "Exploring Deep Dream using Tensorflow 2.0 .",
            "content": "Diving Into Deep Dream using Tensorflow | Towards AI . Exploring Deep Dream using Tensorflow 2.0 . . . Whenever any person hears about Deep Learning or Neural Network the things which first come into their mind are that it‚Äôs used for Object Detection, Face Recognition, Natural Language Processing, and Speech Recognition. But Neural Network is also capable of generating images. And one of the state-of-the-art methods is called Deep Dream. . What is it? . Deep Dream is a computer vision program created by Google engineer Alex Mordvintsev which uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a Dream-like hallucinogenic appearance in the deliberately over-processed images. . Some images which is generated using Deep Dream . . . . . How does it work? . In simple terms, many levels of neural networks process the images input into the program. The artificial neurons are calculated and the weight of their sum processed through the roughly three-layered network: low, intermediate, and high-level layers. The lower levels are responsible for more basic edges, corners, and textures. By maximizing those levels, the picture would end up looking more like a Van Gogh. The higher levels are responsible for more detailed, hierarchical input like buildings and other elaborate objects. When the higher levels are maximized, the picture looks more like a jumbled Dali. . Let us create our first simple Deep Dream. . In this tutorial, we‚Äôre going to use Tensorflow 2.0 and we run it on Google Colab. In the following 6 steps, we‚Äôre going to build our first deep dream model. So let‚Äôs get started. . 1. Importing all dependencies . Here we‚Äôre going to use Indian actress Deepika Padukone image and then preproccess it. . output:- . . 2. Prepare the feature extraction model . Download and prepare a pre-trained image classification model. You will use InceptionV3 which is similar to the model originally used in DeepDream. The InceptionV3 architecture is quite large (for a graph of the model architecture see TensorFlow‚Äôs research repo). For DeepDream, the layers of interest are those where the convolutions are concatenated. There are 11 of these layers in InceptionV3, named ‚Äòmixed0‚Äô though ‚Äòmixed10‚Äô. Using different layers will result in different dream-like images. Deeper layers respond to higher-level features (such as eyes and faces), while earlier layers respond to simpler features (such as edges, shapes, and textures). Feel free to experiment with the layers selected below, but keep in mind that deeper layers (those with a higher index) will take longer to train on since the gradient computation is deeper. . 3. Calculate loss . The loss is the sum of the activations in the chosen layers. The loss is normalized at each layer so the contribution from larger layers does not outweigh smaller layers. . 4. Gradient ascent . Once you have calculated the loss for the chosen layers, all that is left is to calculate the gradients with respect to the image and add them to the original image. Adding the gradients to the image enhances the patterns seen by the network. At each step, you will have created an image that increasingly excites the activations of certain layers in the network. . OUTPUT:- . . 5. Taking it up an octave . Pretty good, but there are a few issues with this first attempt: (a) The output is noisy (this could be addressed with a tf.image.total_variation loss). (b)The image is low resolution. (c)The patterns appear like they‚Äôre all happening at the same granularity. To overcome these issues we can perform the previous gradient ascent approach, then increase the size of the image (which is referred to as an octave), and repeat this process for multiple octaves. . OUTPUT:- . . Hurray, we‚Äôve just generated an image using Deep Dream. In, case you don‚Äôt like to code the Deep Dream algorithm manually but want to create images with Deep Dream then here is the solution. You can use DeepDreamGenerator. . LINK:- Deep Dream Generator The technique is a much more advanced version of the original Deep Dream approach. It is capable of using its own‚Ä¶deepdreamgenerator.com . Note:- In the above example some lines of codes are not showing because in Medium the first 11 lines of GitHub gist are only displayed. So I strongly suggest you to download colab notebook(.ipnyb file) from my GitHub repo. . Link of my Deep_Dream repo:- abhinavsp0730/Deep_Dream Deep Dream model of my Medium Blog. Contribute to abhinavsp0730/Deep_Dream development by creating an account on‚Ä¶github.com . Refrences: Tensorflow‚Äôs official Documentaion . Do you know what, you can hit the clap button 50 times in medium? If you like this blog, show some love by doing claps. . . THANK YOU. .",
            "url": "https://abhinavsp0730.github.io/neurlap/markdown/2019/09/14/Exploring-Deep-Dream-using-Tensorflow-2.html",
            "relUrl": "/markdown/2019/09/14/Exploring-Deep-Dream-using-Tensorflow-2.html",
            "date": " ‚Ä¢ Sep 14, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "Understanding the Backpropagation Algorithm",
            "content": "Intro to Backpropagation 101 | Towards AI . Understanding the Backpropagation Algorithm . Easily understand the backpropagation algorithm in a way you‚Äôve never before! . You might have been taking a course on deep learning and at the beginning of it, it seems to you very easy, and then you have encountered ‚Äúbackpropagation‚Äù and you will start your head scratching because it is too ‚Äúmathsy.‚Äù . . ‚Äúwhy there is a need for understanding the Back Propagation algorithm for us? The answer is very simple. We, human beings are always curious to know how things are happening in the real world. For example, when we see human breathing. From outside it looks like it is simply the intake of air &amp; release of CO2. . But, the curiosity of the human race led us, to know that, in this short interval of 1‚Äì2 sec. The whole blood id first oxygenated &amp; then it transported through nerves and reaches to every single cell of our body. And finally, it deoxygenated. . Humans are always curious to know, how the mechanism which governs the things of the real world around us. And for any Deep Learning practitioners our,‚Äù Our world revolves around neural networks‚Äù. So, for the sake of killing your curiosity &amp; and giving you the idea of how neural networks are trained. I‚Äôm presenting you with this article. . The context of this article are:- . Giving you a brief intro about the neural networks. | Try to make you understand Back Propagation in a simpler way. | And, finally, we‚Äôll deal with the algorithm of Back Propagation with a concrete example. | Okay! So, first understand what is a neural network. . I don‚Äôt know you are aware of a neural network or not. So let us first understand this concept. . . A neural network is a series of algorithms that endeavors to recognize, underlying relationships in a set of data through a process that mimics the way the human brain operates. Neural networks can adapt to changing input; so the network generates the best possible result without needing to redesign the output criteria. . or in a simple words, we can describe a neural networks as:- . -It‚Äôs just a computer program that learn and behave in a remarkably similar way to human brains. . Or, it‚Äôs just a way how computers learn things, recognize patterns, and make decisions in a human like way. . | Or, neural networks enable computers to learn from a given sent of data. . | . -Or, ( in a more elaborative way ) An ANN is simulation of the network of neurons that make up a human brain so that the computer will be able to learn things and make decisions in a human like manner. . In simpler words, Back Propagation is the central mechanism to train a neural network. In which we calculate the error of our desired targeted output value &amp; then we adjust the weights in a way to minimize this error. it‚Äôs similar to human being how we learn from our mistakes. . Try to imagine the situation. You are trying to hit a football into the goal post. you randomly kick the football with some angle. Then you measure the distance from the goalpost to the spot where your football goes in the first attempt. Then we try to minimize this distance by changing the angle by which we kicked the football. And finally, we‚Äôre able to hit the football into the goal post. . Now, compare this situation with a neural network:- . we randomly kicked the football. In a neural network, we initialized the weights randomly. . | we measure the distance from the goal post &amp; the spot where we hit the ball.‚Äô In a neural network, this is called measuring the total error. . | We changed the angle in such a manner to minimize this distance . | In a neural network, this is called updating the weights in order to get the desired targeted output. . The Backpropagation algorithm is a very powerful algorithm in order to train a neural network. it‚Äôs so powerful that it is used in Zip Code recognition(low-level example),Face recognition (mid-level example) to Sonar target recognition(high-level example). . I hope now you‚Äôve understood Back Propagation. So, now you are ready to deal with the mathematical stuffs of Back Propagation. . Okay so, now jump into Backpropagation algorithm to understand it. . . This is a figure of a simple neural network having 2 layers i.e input, hidden and output layer, respectively. Each layer is having 2 neurons. . . The function of a neuron is, to sum up, all the multiplied inputs with its weight &amp; the bias. And the Output is followed by the operation of the activation function. . Let us understand Back Propagation with an example: . . . Here,H1 is a neuron and the sample inputs are x1=0.05,x2=0.10 and the biases are b1=0.35 &amp; b2=0.60. The targeted values are T1=0.01 &amp; T2=0.22 . Now we randomly initialize the weights, . . Note: In this whole article we‚Äôre using SIGMOID as an activation function. . . Let us calculate H1, H2 and output H1, output H2. . . Similarly, we can calculate y1,y2, output y1 &amp; output y2. . . Calculating the total error . . Now we have to backpropagate, to upgrade the weights . Consider w5, Error at w5 . . But there is no term w5 present in the expression of Etotal. So we have to split it &amp; apply the chain rule to partially differentiate it. . Splitting . . Partially differentiating each term one by one. . . . . Calculated error w5:- . . Now updating w5 . . New updated weights,w5=0.3595 &amp; similarly w6=0.4086,w7=0.511 &amp; w8=0.561 . . Now at hidden layer,updating w1,w2,w3&amp;w4. . Consider w1 Error at w1 . . But there is no w1 term present in the expression of Etotal. So, in order to do that, we have multiple splits. . Please pay attention and look it slowly! . The terms which are encircled, we can‚Äôt differentiate them directly. So, we have to split them. . . Consider the term which is encircled orange &amp; let us split &amp; apply the chain rule. . . Now calculating: . . . We have calculated the term which is encircled orange. . . Consider the term which is encircled in blue. . . Consider the term which is encircled in black. . . Now we‚Äôve calculated all the terms which are encircled. . Therefore the error at w1 is: . . Updating w1: . . Now, we are having our updated weight w1 and similarly, we can calculate w2,w3 &amp;w4. What we‚Äôve done so far is we Back-Propagated and updated first w5,w6,w7,w8 and then with the help these we further Back-Propagated and updated the weights w1,w2,w3 &amp; w4. . So with these updated weights(w1,w2,w3 &amp; w4).We‚Äôve to again calculate H1 &amp; H2.After calculating H1 &amp; H2 ,we can calculate y1 output &amp; y2 output. After that, we can calculate the Total Error as we have done earlier, and again with the help of this new Total Error. We backpropagate &amp; updated the weights. . . And again with the updated weights we forward propagate. . . We‚Äôve to iterate over &amp; over again between Back Propagation &amp; forward propagation. . . . . Until the Total Error(cost function) is minimized or in other words, the value of our predicted outputs is closer to that of the target values. . In one sentence we can define backpropagation *as *it‚Äôs a common method of training a neural net in which the initial system output is compared to the desired output, and the system is adjusted until the difference between the two is minimized. . We can now say that backpropagation is the central mechanism to train any neural network. . I hope you‚Äôve understood now what is the backpropagation algorithm and how it works. . Congratulations you‚Äôve just understood one of the toughest ‚Äúmathsy‚Äù topics of machine learning. . Don‚Äôt forget to give us your üëè &amp; follow me!!!!!!! . .",
            "url": "https://abhinavsp0730.github.io/neurlap/2019/07/18/Understanding-the-Backpropagation-Algorithm.html",
            "relUrl": "/2019/07/18/Understanding-the-Backpropagation-Algorithm.html",
            "date": " ‚Ä¢ Jul 18, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "How to Use scikit-learn ‚Äòeli5‚Äô Library to Compute Permutation Importance?",
            "content": "Feature Permutation Importance with ‚Äòeli5‚Äô | Towards AI . How to Use scikit-learn ‚Äòeli5‚Äô Library to Compute Permutation Importance? . Understanding the workings of scikit-learn‚Äôs ‚Äòeli5‚Äô library to compute feature importance on a sample housing dataset and interpreting its results . . Most of the Data Scientist(ML guys) treat their machine learning model as a black-box. They don‚Äôt know what are the things which are happening underhood. They load their data, do manual data cleaning &amp; prepare their data to fit it on ml modal. Then the train their model &amp; predict the target values(regression problem). . But they don‚Äôt know, what features does their model think are important? . . For answering the above question Permutation Importance comes into the picture. . What is it? . Permutation Importance is an algorithm that computes importance scores for each of the feature variables of a dataset, The importance measures are determined by computing the sensitivity of a model to random permutations of feature values. . How does it work? . The concept is really straightforward: We measure the importance of a feature by calculating the increase in the model‚Äôs prediction error after permuting the feature. A feature is ‚Äúimportant‚Äù if shuffling its values increases the model error because in this case, the model relied on the feature for the prediction. A feature is ‚Äúunimportant‚Äù if shuffling its values leave the model error unchanged because in this case, the model ignored the feature for the prediction. . Should I compute importance on Training or Test data(validation data)? . The answer to this question is, we always measure permutation importance on test data. permutation importance based on training data is garbage. The permutation importance based on training data makes us mistakenly believe that features are important for the predictions when in reality the model was just overfitting and the features were not important at all. . eli5 ‚Äî a scikit-learn library:- . eli5 is a scikit learn library, used for computing permutation importance. . caution to take before using eli5:- . **1. **Permutation Importance is calculated after a model has been fitted. . **2. **We always compute permutation importance on test data(Validation Data). . **3. **The output of eli5 is in HTML format. So, we can only use it in the ipython notebook(i.e Jupiter notebook, google collab &amp; kaggle kernel, etc). . Now, let us get some test of codes üòã . . I‚Äôve built a rudimentary model(RandomForestRegressor) to predict the sale price of the housing data set. This is a good dataset example for showing the Permutation Importance because this dataset has a lot of features. So, we can see which features make an impact while predicting the values and which are not. . Now, we use the ‚Äòeli5‚Äô library to calculate Permutation importance. . you can see the output of the above code below:- . . Interpreting Results:- . Features have decreasing importance in top-down order. The first number in each row shows the reduction in model performance by the reshuffle of that feature. The second number is a measure of the randomness of the performance reduction for different reshuffles of the feature column. overallQual(overall quality) feature of the housing data set makes the biggest impact in the model while predicting the Sale Price. . You can get the housing-data set in .csv format from my GitHub profile . LINK:- https://github.com/abhinavsp0730/housing_data/blob/master/home-data-for-ml-course.zip . You can also get .ipnyb file(kaggle Kernel) file from my GitHub profile . LINK:- . https://github.com/abhinavsp0730/housing_data/blob/master/kernel659579854a(2).ipynb . THANK YOU . If you enjoy my article then do claps and follow me ‚ù§Ô∏è. . .",
            "url": "https://abhinavsp0730.github.io/neurlap/markdown/2019/07/07/How-to-Use-scikit-learn-eli5-Library-to-Compute-Permutation-Importance.html",
            "relUrl": "/markdown/2019/07/07/How-to-Use-scikit-learn-eli5-Library-to-Compute-Permutation-Importance.html",
            "date": " ‚Ä¢ Jul 7, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Articles",
          "content": "I write deeplearning/machine learning blogs on medium under the famous publications: . Towards AI. | Analytics Vidya. Here is my all published articles, you can read these blogs on medium or in this site. . | Tensorflow Callbacks in action . Read on Medium | Read on this site | . | Tf.estimator, a Tensorflow High-level API . Read on Medium | Read on this site | . | Exploring Deep Dream using Tensorflow 2.0 . Read on Medium | Read on this site | . | How to Use scikit-learn ‚Äòeli5‚Äô Library to Compute Permutation Importance? . Read on Medium | Read on this site | . | Understanding the Backpropagation Algorithm . Read on Medium | Read on this site | . | .",
          "url": "https://abhinavsp0730.github.io/neurlap/articles/",
          "relUrl": "/articles/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Projects",
          "content": "This page contains all of my Projects . youtube: https://youtu.be/yQ-4SiZxMNM . Project Antivenom . Project AntiVenom is an innovative idea of bringing technology into the aid of human problems. Currently, region-specific for the Indian state of Odisha, this project aims at improving the disastrous situation between humans and snake There is no pre-made snake dataset present on the internet. So I‚Äôve prepared the dataset. Then I performed transfer learning using tensorflow 2.0 and its high-level API keras and using mobile net model due to its lightweight nature. Then this model is able to classify the snake‚Äôs breed and whether it‚Äôs venomous or not. Then using Tensorflow Lite I served this model into android for production. Also, this project is featured in the newspaper: Link Android app Playstore link: Link Visit the website: Link . | Xiken.tech . About this Project An organization driven by young minds who don‚Äôt only focus on problems around us, but believe in the concept of ‚ÄúInnovate To Solve‚Äù | Our Mission With enthusiasm and compassion towards our planet and its people, we are not here just to complain about the problems, we are here to solve and simplify. Our projects will be for this world and its inhabitants, and as we proceed, we want to indulge as many people as we can into our mission. | Our Plan Beginning this journey of innovation, we will be launching ‚ÄúProject AntiVenom‚Äù. After which, similar projects with the motive of ‚ÄúInnovate to Solve‚Äù will follow. At the same time, we will be open to collaborations. Our projects will be open sourced, so that everyone who has the urge of ‚ÄúInnovate To Solve‚Äù can contribute. | Our Vision We believe that with the help of technology, every problem can be solved. Therefore, we want to innovate solutions which will help change the environment around us, and the world as a whole, one step at a time. We might be young, but we sure have the balls to make an impact. Project Link | . | .",
          "url": "https://abhinavsp0730.github.io/neurlap/project/",
          "relUrl": "/project/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Tweets",
          "content": "twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 .",
          "url": "https://abhinavsp0730.github.io/neurlap/tweets/",
          "relUrl": "/tweets/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "About Me",
          "content": "Give me the data and I‚Äôll show you its true potential. I let computers see with the help of neural networks. I build End-to-End Machine Learning products. I know how to use data efficiently. Give me data and I‚Äôll turn into them live projects. Helping humanity with the help of AI with this aim founded xiken.tech . I‚Äôm the lead of state-level project name ProjectAntivenom. I‚Äôm a speaker and a blogger who loves to help the community, who believes knowledge is of no use. if you don‚Äôt share and apply it. Reach out to me for spreading the knowledge of AI into your community events. I‚Äôm the co-organizer of PyData Bhubaneswar Chapter. ‚ö°‚ö°‚ö°‚ö°‚ö° I‚Äôm open for internship opportunity and freelancing project ‚ö°‚ö°‚ö°‚ö°‚ö°‚ö°‚ö°‚ö° üõ†Ô∏è Construct Ai solution for your real-world problem. üõ†Ô∏è Interested In to build Ai solution for Health Care . üõ†Ô∏è Looking For internship and freelancing project. üõ†Ô∏è Drop an Email to abhinavsp0730@gmail.com for further discussion. .",
          "url": "https://abhinavsp0730.github.io/neurlap/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "Achievements",
          "content": "Got Feature in the Odisha‚Äôs biggest NewsPaper for my ProjectAntivenom . . | Became the Co-organiser of Py-Data BBSR . | Won TweetsOnTf competion and got featured in TFUGK Website . | Founded **Xiken.tech** . | I‚Äôve Lead a state level Deep Learning project : ProjectAntivenom . | .",
          "url": "https://abhinavsp0730.github.io/neurlap/achievements/",
          "relUrl": "/achievements/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page14": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://abhinavsp0730.github.io/neurlap/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}